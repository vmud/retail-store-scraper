version: '3.8'

services:
  # Main scraper service
  scraper:
    build: .
    container_name: retail-scraper
    command: ["python", "run.py", "--all", "--resume"]
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
      # Oxylabs proxy credentials (optional)
      # Set these for proxy mode, or use --proxy direct for no proxy
      #
      # Residential Proxy credentials
      - OXYLABS_RESIDENTIAL_USERNAME=${OXYLABS_RESIDENTIAL_USERNAME:-}
      - OXYLABS_RESIDENTIAL_PASSWORD=${OXYLABS_RESIDENTIAL_PASSWORD:-}
      # Web Scraper API credentials
      - OXYLABS_SCRAPER_API_USERNAME=${OXYLABS_SCRAPER_API_USERNAME:-}
      - OXYLABS_SCRAPER_API_PASSWORD=${OXYLABS_SCRAPER_API_PASSWORD:-}
      # Legacy/fallback credentials (used if mode-specific not set)
      - OXYLABS_USERNAME=${OXYLABS_USERNAME:-}
      - OXYLABS_PASSWORD=${OXYLABS_PASSWORD:-}
      # Proxy mode: direct, residential, web_scraper_api
      - PROXY_MODE=${PROXY_MODE:-direct}
    restart: unless-stopped
    # Override command for different modes:
    # docker-compose run scraper python run.py --retailer verizon
    # docker-compose run scraper python run.py --all --test --limit 10
    # docker-compose run scraper python run.py --all --proxy residential

  # Dashboard service for monitoring
  dashboard:
    build: .
    container_name: retail-scraper-dashboard
    # Ensure data directory exists before starting (#105)
    command: ["sh", "-c", "mkdir -p /app/data && python dashboard/app.py"]
    ports:
      # Dashboard listens on port 5001 (#105)
      - "5001:5001"
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - FLASK_ENV=production
      - PYTHONUNBUFFERED=1
      # Optional API key for authentication (#92)
      - DASHBOARD_API_KEY=${DASHBOARD_API_KEY:-}
    restart: unless-stopped
    # Removed depends_on: scraper - dashboard can run independently (#105)
    # Health check for dashboard service (#115)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Single retailer services (optional, for running specific retailers)
  verizon:
    build: .
    container_name: retail-scraper-verizon
    command: python run.py --retailer verizon --resume
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
    profiles:
      - retailers

  att:
    build: .
    container_name: retail-scraper-att
    command: python run.py --retailer att --resume
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
    profiles:
      - retailers

  target:
    build: .
    container_name: retail-scraper-target
    command: python run.py --retailer target --resume
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
    profiles:
      - retailers

  tmobile:
    build: .
    container_name: retail-scraper-tmobile
    command: python run.py --retailer tmobile --resume
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
    profiles:
      - retailers

  walmart:
    build: .
    container_name: retail-scraper-walmart
    command: python run.py --retailer walmart --resume
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
    profiles:
      - retailers

  bestbuy:
    build: .
    container_name: retail-scraper-bestbuy
    command: python run.py --retailer bestbuy --resume
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
    profiles:
      - retailers

volumes:
  scraper-data:
  scraper-logs:
